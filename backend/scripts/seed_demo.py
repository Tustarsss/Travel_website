"""Seed the SQLite database with generated demo data."""

from __future__ import annotations

import argparse
import json
from datetime import datetime, timezone
from pathlib import Path
from typing import Sequence

from sqlmodel import SQLModel, delete

from app.core.db import get_session_maker, init_db
from app.models.diaries import Diary, DiaryRating
from app.models.enums import (
    BuildingCategory,
    DiaryMediaType,
    DiaryStatus,
    FacilityCategory,
    RegionType,
    TransportMode,
)
from app.models.graph import GraphEdge, GraphNode
from app.models.locations import Building, Facility, Region
from app.models.users import User

DATA_FILES = {
    "regions": Region,
    "buildings": Building,
    "facilities": Facility,
    "graph_nodes": GraphNode,
    "graph_edges": GraphEdge,
    "users": User,
    "diaries": Diary,
    "diary_ratings": DiaryRating,
}

ENUM_FIELDS: dict[type[SQLModel], dict[str, type]] = {
    Region: {"type": RegionType},
    Building: {"category": BuildingCategory},
    Facility: {"category": FacilityCategory},
    GraphEdge: {"transport_modes": TransportMode},
    Diary: {"media_types": DiaryMediaType, "status": DiaryStatus},
}


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Seed the database with generated data")
    parser.add_argument(
        "--dataset",
        type=Path,
        default=Path("data/generated"),
        help="Directory containing JSON files generated by scripts/generate_data.py",
    )
    parser.add_argument(
        "--drop",
        action="store_true",
        help="Drop existing rows before inserting new demo data.",
    )
    return parser.parse_args()


def _load_json(path: Path) -> Sequence[dict]:
    if not path.exists():
        raise FileNotFoundError(f"Dataset file not found: {path}")
    return json.loads(path.read_text(encoding="utf-8"))


def _coerce_enums(model_cls: type[SQLModel], payload: dict) -> dict:
    enum_map = ENUM_FIELDS.get(model_cls)
    if not enum_map:
        return payload

    converted = payload.copy()
    for field_name, enum_type in enum_map.items():
        value = converted.get(field_name)
        if value is None:
            continue
        if isinstance(value, list):
            converted[field_name] = [enum_type(item) for item in value]
        else:
            converted[field_name] = enum_type(value)
    return converted


async def seed_database(dataset_dir: Path, drop_existing: bool) -> None:
    maker = get_session_maker()
    async with maker() as session:
        if drop_existing:
            print("[seed-demo] Dropping existing data...")
            for model in (
                DiaryRating,
                Diary,
                GraphEdge,
                GraphNode,
                Facility,
                Building,
                Region,
                User,
            ):
                await session.execute(delete(model))
            await session.commit()

        for name, model_cls in DATA_FILES.items():
            data_path = dataset_dir / f"{name}.json"
            records = _load_json(data_path)
            print(f"[seed-demo] Seeding {name} ({len(records)} records)...")
            objects = [model_cls(**_coerce_enums(model_cls, data)) for data in records]
            session.add_all(objects)
            await session.commit()


def ensure_directories() -> None:
    for path in (Path("storage"), Path("indexes"), Path("indexes/map_tiles")):
        path.mkdir(parents=True, exist_ok=True)
    spatial = Path("indexes/spatial.idx")
    fulltext = Path("indexes/fulltext.idx")
    if not spatial.exists():
        spatial.write_text("", encoding="utf-8")
    if not fulltext.exists():
        fulltext.write_text("", encoding="utf-8")


def export_geojson_tiles(dataset_dir: Path) -> None:
    tiles_dir = Path("indexes/map_tiles")
    tiles_dir.mkdir(parents=True, exist_ok=True)

    # Clear old tiles
    for tile in tiles_dir.glob("region_*.geojson"):
        tile.unlink()

    regions = _load_json(dataset_dir / "regions.json")
    nodes = _load_json(dataset_dir / "graph_nodes.json")
    edges = _load_json(dataset_dir / "graph_edges.json")

    nodes_by_id = {node["id"]: node for node in nodes}
    nodes_by_region: dict[int, list[dict]] = {}
    for node in nodes:
        nodes_by_region.setdefault(node["region_id"], []).append(node)

    edges_by_region: dict[int, list[dict]] = {}
    for edge in edges:
        edges_by_region.setdefault(edge["region_id"], []).append(edge)

    timestamp = datetime.now(timezone.utc).isoformat()
    index_payload = {"tiles": []}

    for region in regions:
        region_id = region["id"]
        features: list[dict] = []

        for edge in edges_by_region.get(region_id, []):
            start = nodes_by_id.get(edge["start_node_id"])
            end = nodes_by_id.get(edge["end_node_id"])
            if not start or not end:
                continue
            features.append(
                {
                    "type": "Feature",
                    "geometry": {
                        "type": "LineString",
                        "coordinates": [
                            [start["longitude"], start["latitude"]],
                            [end["longitude"], end["latitude"]],
                        ],
                    },
                    "properties": {
                        "feature_type": "edge",
                        "distance": edge.get("distance"),
                        "transport_modes": edge.get("transport_modes", []),
                    },
                }
            )

        for node in nodes_by_region.get(region_id, []):
            node_type = "poi"
            if node.get("building_id"):
                node_type = "building"
            elif node.get("facility_id"):
                node_type = "facility"

            features.append(
                {
                    "type": "Feature",
                    "geometry": {
                        "type": "Point",
                        "coordinates": [node["longitude"], node["latitude"]],
                    },
                    "properties": {
                        "feature_type": node_type,
                        "name": node.get("name"),
                        "building_id": node.get("building_id"),
                        "facility_id": node.get("facility_id"),
                    },
                }
            )

        tile_path = tiles_dir / f"region_{region_id}.geojson"
        tile_path.write_text(
            json.dumps({"type": "FeatureCollection", "features": features}, ensure_ascii=False),
            encoding="utf-8",
        )

        index_payload["tiles"].append(
            {
                "region_id": region_id,
                "name": region.get("name"),
                "tile": tile_path.name,
                "updated_at": timestamp,
            }
        )

    (tiles_dir / "index.json").write_text(
        json.dumps(index_payload, ensure_ascii=False, indent=2),
        encoding="utf-8",
    )


def main() -> None:
    args = parse_args()
    dataset_dir: Path = args.dataset
    if not dataset_dir.exists():
        raise FileNotFoundError(f"Dataset directory not found: {dataset_dir}")

    print("[seed-demo] Initializing database schema...")
    init_db()

    ensure_directories()

    print("[seed-demo] Seeding database with demo data...")
    import asyncio

    asyncio.run(seed_database(dataset_dir, drop_existing=args.drop))
    export_geojson_tiles(dataset_dir)
    print("[seed-demo] Demo dataset seeding complete!")


if __name__ == "__main__":
    main()
